{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multiclass_Logistic_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkW+xjXfaGJkjk83o7m7i6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bgalerne/M1MAS_Stat_Images/blob/master/Multiclass_Logistic_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHDqo7H0ymzd",
        "colab_type": "text"
      },
      "source": [
        " # Multiclass logistic regression\n",
        " Goal:\n",
        " 1. Define functions for training a multiclass logistic regression \n",
        " 1. Train the classifier using gradient descent\n",
        " 1. Visualize a multi-class logisitc regression for 2D data\n",
        " 1. (TODO) Implement averaged stochastic gradient descent \n",
        "\n",
        " **Reference:**\n",
        " Section \"4.3.4 Multiclass logistic regression\"\n",
        " of \n",
        " \n",
        " C. M. Bishop *Pattern Recognition and Machine Learning*,\n",
        "Information Science and Statistics, Springer, 2006\n",
        "\n",
        "Freely available:\n",
        "https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2vWdx2AvyGMg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn.datasets import make_moons, make_circles, make_classification, make_blobs, make_gaussian_quantiles\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzWqv9vv0e9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create some toy datasets :\n",
        "\n",
        "#\n",
        "n_class = 3\n",
        "# Three examples of synthetic 2D datasets:\n",
        "X, t = make_blobs(n_features=2, centers = n_class)\n",
        "#X, t = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=24, n_classes=n_class, n_clusters_per_class=1)\n",
        "#X, t = make_gaussian_quantiles(n_features=2, n_classes=n_class)\n",
        "\n",
        "X = StandardScaler().fit_transform(X)\n",
        "print(X.shape)\n",
        "\n",
        "#X, y = make_gaussian_quantiles(n_features=2, n_classes=3)\n",
        "\n",
        "\n",
        "X_train, X_test, t_train, t_test = train_test_split(X, t, test_size=.4, random_state=12)\n",
        "\n",
        "figure = plt.figure(figsize=(10, 10))\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')\n",
        "\n",
        "N_train = X_train.shape[0]\n",
        "N_test = X_test.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8V-HnaFy7uI6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply feature transform:\n",
        "\n",
        "# Add the 1 coordinate for bias:\n",
        "feature_transform = lambda X : np.hstack( (X, np.ones((X.shape[0],1))))\n",
        "# Add the 1 coordinate and square of coordinates:\n",
        "#feature_transform = lambda X : np.hstack( (X, X**2, np.ones((X.shape[0],1))))\n",
        "\n",
        "Phi_train = feature_transform(X_train)\n",
        "n_feat = Phi_train.shape[1]\n",
        "\n",
        "Phi_test = feature_transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwcajoX759ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for Multiclass logistic regression:\n",
        "# - W is the matrix of size n_feat x n_class\n",
        "\n",
        "def soft_max(W, Phi):\n",
        "  # evaluate the softmax vector for a list of feature points phi (given in line) \n",
        "  a = W.transpose() @ Phi.transpose() \n",
        "  y = np.exp(a)\n",
        "  s = np.sum(y,axis=0)\n",
        "  y = (y/s).transpose()\n",
        "  return(y)\n",
        "\n",
        "def predicted_class(W,Phi):\n",
        "  y = soft_max(W, Phi)\n",
        "  if Phi.ndim==1:\n",
        "    pred = np.argmax(y)\n",
        "  else:\n",
        "    pred = np.argmax(y,axis=1)\n",
        "  return(pred)\n",
        "\n",
        "def mloglikelihood(W, Phi, t):\n",
        "  y = soft_max(W, Phi)\n",
        "  # extract values of softmax for the class k=t\n",
        "  if t.ndim == 0:\n",
        "    y = y[t]\n",
        "    L = np.log(y)\n",
        "  else:\n",
        "    y = y[np.arange(Phi.shape[0]),t]\n",
        "    L = - np.sum(np.log(y))\n",
        "  return(L)\n",
        "\n",
        "def gradmloglikelihood(W, Phi, t):\n",
        "  y = soft_max(W, Phi)\n",
        "  # extract values of softmax for the class k=t\n",
        "  if t.ndim == 0:\n",
        "    y[t] -= 1\n",
        "    y.shape = (n_class,1)\n",
        "    Phi.shape = (1,n_feat)\n",
        "    g = y @ Phi\n",
        "  else:\n",
        "    y[np.arange(Phi.shape[0]),t] = y[np.arange(Phi.shape[0]),t] - 1\n",
        "    #print(y.shape)\n",
        "    #print(Phi.shape)\n",
        "    g = y.transpose() @ Phi\n",
        "  return(g.transpose())\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOXP7T3aaxrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NChi0BqOsvz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random initialization:\n",
        "W = np.random.random((n_feat,n_class))\n",
        "\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "Nit = 10**4\n",
        "for n in range(Nit):\n",
        "  W -= lr*gradmloglikelihood(W, Phi_train, t_train)\n",
        "  if n%1000==0:\n",
        "    print(mloglikelihood(W, Phi_train, t_train))\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrtNLDeP3yUj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pred = predicted_class(W,Phi_test)\n",
        "print(pred[:30])\n",
        "print(t_test[:30])\n",
        "# TODO: Compute accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRRxPsI2Amdd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualize results:\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
        "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
        "h = 0.02\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                        np.arange(y_min, y_max, h))\n",
        "X_grid = np.hstack((xx.ravel(), yy.ravel()))\n",
        "print(X_grid.shape)\n",
        "N_grid = xx.ravel().shape[0]\n",
        "X_grid = np.c_[xx.ravel(), yy.ravel()]\n",
        "Phi_grid = feature_transform(X_grid)\n",
        "print(Phi_grid.shape)\n",
        "Z = predicted_class(W,Phi_grid)\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "figure = plt.figure(figsize=(16, 8))\n",
        "ax = plt.subplot(1,2,1)\n",
        "ax.set_title(\"Input data\")\n",
        "ax.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
        "ax.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')\n",
        "ax = plt.subplot(1,2,2)\n",
        "cmap = ListedColormap(['b','y','r'])\n",
        "plt.contourf(xx,yy,Z,  cmap = cmap, alpha=.8)\n",
        "ax.scatter(X_train[:, 0], X_train[:, 1], marker='o', c=t_train, s=50, edgecolor='k')\n",
        "ax.scatter(X_test[:, 0], X_test[:, 1], marker='P', c=t_test, s=50, edgecolor='k')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8AjzL2drN90",
        "colab_type": "text"
      },
      "source": [
        "Implement an average stochastic gradient descent for solving the optimization problem:\n",
        "At each step sort one example of the dataset and use it for a noisy gradient estimate.\n",
        "Compute the new weight $W^{(n)}$ and the average weights $\\bar{W}^{(n)}$ defined by\n",
        "$$\n",
        "\\bar{W}^{(n)} = \\frac{1}{n+1} \\sum_{k=0}^{n} W^{(k)} = \\frac{n}{n+1}\\bar{W}^{(n-1)} + \\frac{1}{n+1}W^{(n)}.\n",
        "$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDtalrY_rM3k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98wStaWlsAFb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BBVocwLsAb_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell for tests\n",
        "print(W)\n",
        "print('Tests of soft_max')\n",
        "print(soft_max(W, Phi_train[0,:]))\n",
        "print(soft_max(W, Phi_train[1,:]))\n",
        "print(soft_max(W, Phi_train[0:2,:]))\n",
        "print('\\nTests of mloglikelihood')\n",
        "print(mloglikelihood(W, Phi_train[0,:], t_train[0]))\n",
        "print(mloglikelihood(W, Phi_train[0:2,:], t_train[0:2]))\n",
        "print(mloglikelihood(W, Phi_train, t_train))\n",
        "print('\\nTests of gradmloglikelihood')\n",
        "print(gradmloglikelihood(W, Phi_train, t_train))\n",
        "print(gradmloglikelihood(W, Phi_train[0,:], t_train[0]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}